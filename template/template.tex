\newif \iftheory 

\theorytrue %\theoryfalse

\newif \ifblind

\blindtrue \blindfalse

\newcommand{\eli}[1]{\textcolor{blue}{[Eli: #1]}}
\newcommand{\meg}[1]{\textcolor{red}{[Megumi: #1]}}
\newcommand{\hannah}[1]{\textcolor{magenta}{[Hannah: #1]}}


% ~~~~~ Modify inside here ~~~~~%
\newcommand{\authorlist}{
% Author 1
Megumi Ando\iftheory\thanks{Computer Science Department, Tufts University, {\tt mando@cs.tufts.edu}}\fi
\and 
% Author 2
Anna Lysyanskaya\iftheory\thanks{Computer Science Department, Brown University, {\tt anna@cs.brown.edu}}\fi
\and 
% Author 3
Hannah Marsh\iftheory\thanks{Computer Science Department, Tufts University, {\tt hannah@cs.tufts.edu}}\fi
\and
% Author 3
Eli Upfal\iftheory\thanks{Computer Science Department, Brown University, {\tt eli\_upfal@brown.edu}}\fi
}

\newcommand{\institutelist}{}


\newcommand{\titlelist}{\iftheory\begin{bf}\fi
% title
Onion Routing on Sparse Distributed Networks
\iftheory\end{bf}\fi}

% Path to stylefiles
\newcommand{\pathstyles}{./stylefiles}

% Path to preambles
\newcommand{\pathpreambles}{./preambles}

% Path to bibfiles
\newcommand{\pathbibs}{./bibfiles}

% ~~~~~ Modify inside here ~~~~~%


\iftheory
% PLAIN ARTICLE
\documentclass[11pt]{article}

% LNCS (DEFAULT)
\else
%\documentclass[envcountsame, 10pt]{\pathstyles/llncs}
\documentclass[runningheads,a4paper]{llncs}
\ifblind 
\institute{}
\else
\institute{\institutelist}
\fi
\fi

\ifblind 
\author{}
\else
\author{\authorlist} 
\fi
\pagestyle{plain}

\title{\titlelist}

% PREAMBLE
\input{\pathpreambles/preamble}


% ~~~ Begin document ~~~ %
\begin{document}

\maketitle
\iftheory
\thispagestyle{empty}
\fi

% ABSTRACT
\vspace{-8mm}
\begin{abstract}
In distributed networks, an adversary with full network visibility may analyze traffic patterns to infer who is communicating with whom. Anonymous communication protocols provide a necessary defense by ensuring that any two observable communication patterns are statistically indistinguishable. One of the most practical and widely adopted methods for establishing anonymous communication channels is onion routing, where messages are encrypted in layers and relayed through multiple intermediaries. Ando, Lysyanskaya, and Upfal [ICALP 2018] showed that in a setting where each participant sends and receives exactly one fixed-length message, onion routing achieves anonymity against the network adversary when intermediaries are selected uniformly at random, and both server load and path length are polylogarithmic in the security parameter.

This result holds when the network topology resembles a complete graph. We aim to extend this to the sparse network setting where each server is connected directly to only a few neighbors. In particular, we are interested in $r$-regular expander graphs. We show that if each routing path is determined by an independent, fixed-length random walk, our protocol achieves anonymity against the network adversary, with both server load and path length remaining polylogarithmic in the security parameter.
\iftheory
\vfill
%\noindent\textbf{Keywords:} 
%Keyword, 
%keyword, 
%keyword.
\fi
\end{abstract}
 

% ~~~ BODY ~~~ %
\iftheory
\newpage
\thispagestyle{empty}
\newpage
\setcounter{page}{1}
\fi

\newcommand{\parties}{\mathsf{Parties}}
\newcommand{\keygen}{\mathsf{KeyGen}}
\newcommand{\formonion}{\mathsf{FormOnion}}
\newcommand{\proconion}{\mathsf{PeelOnion}}
\newcommand{\proconionhelper}{\mathsf{PeelOnionHelper}}
\newcommand{\bruiseonion}{\mathsf{BruiseOnion}}
\newcommand{\polylog}{\mathsf{polylog}}
\newcommand{\early}{\mathsf{early}}
\newcommand{\late}{\mathsf{late}}
\newcommand{\recipient}{\mathsf{Recipient}}
\newcommand{\Bad}{\mathsf{Bad}}

\section{Introduction}

Protecting user privacy in communication is a growing challenge in modern networks. Anonymous communication channels are essential for preventing adversaries from linking users to their activities, whether it's for online privacy, whistleblowing, or secure voting. One well-known method for achieving anonymous communication is \textit{onion routing}, where messages are wrapped in layers of encryption and routed through a series of intermediaries before reaching the intended recipient. Each intermediary decrypts a single layer, revealing only the next node in the chain and the ciphertext to forward. 

Onion routing is widely adopted due to its simplicity and ability to tolerate failures in a subset of nodes. Systems like Tor \cite{tor}, which implement onion routing, are used by millions of people daily to protect their online activities. However, despite its widespread use, the security guarantees of onion routing are not fully understood, especially in the face of powerful adversaries.

Previous work, such as the protocol $\Pi_p$ introduced in~\cite{ICALP:AndLysUpf18}, shows that \textit{statistical privacy} can be achieved against a passive adversary if the intermediaries in the network are chosen uniformly at random from all available nodes in a fully connected network. However, this approach relies on a complete graph, where every node can communicate directly with every other node, leading to a quadratic increase in the number of network edges as the system scales to accommodate more nodes. Thus the fully connected network is inefficient for large systems.

We present the design and evaluation of $\Pi_x$, a new protocol that aims to provide similar levels of anonymity while requiring significantly fewer network edges. Instead of relying on a fully connected network, $\Pi_x$ assumes an \textit{expander} topology, a class of sparse graphs that maintain strong connectivity even with very few edges. In our approach, each node has a probability distribution based on its connections to a fixed number of other nodes, from which clients select the next hop in their routing path. We demonstrate that, by configuring these distributions to approximate an expander graph, $\Pi_x$ can provide the same anonymity guarantees as a fully connected network but with only $O(n)$ edges.

We demonstrate that expander graphs can be a practical foundation for large-scale anonymous communication systems, providing a balance between efficiency and privacy.


% \section{Definitions}

% \subsection{Security Parameter}

% The security parameter\footnote{It is worth noting that ``$\lambda$'' is also commonly used to denote eigenvalues (of a graph's adjacency matrix). To avoid confusion, we will explicitly specify whenever $\lambda$ represents something other than the security parameter.}, $\lambda \in \mathbb{N}$, is a tunable input that quantifies a system's level of security.

% \begin{definition} [Negligibility] \label{def:negl}
%     A function $f : \mathbb{N} \rightarrow \mathbb{R}$ is negligible in $\lambda$, written $f(\lambda) = \negl$, if for every polynomial~$p(\cdot)$ and all sufficiently large $\lambda$, 
%     $$
%     f(\lambda) < \frac{1}{p(\lambda)}.
%     $$
% \end{definition}

% \begin{definition} [Overwhelming Probability] \label{def:wop}
%     An event occurs with overwhelming probability if it is the complement of an event with probability negligible in $\lambda$.
% \end{definition}

% \begin{definition} [Statistical Closeness] \label{def:statistical-close}
%     We say that two families of distributions~$\{D_{0, \lambda}\}_{\lambda \in\mathbb{N}}$ and $\{D_{1, \lambda}\}_{\lambda \in\mathbb{N}}$ are statistically close if the statistical distance between $D_{0, \lambda}$ and $D_{1, \lambda}$ is negligible in $\lambda$~\footnote{When the security parameter is clear by context, we abbreviate this notion by $D_0 \approx_s D_1$.}.
% \end{definition}

% \subsection{Network and Participants}

% Our setup requires two distinct roles: $N$ clients (users), who wish to send private messages to other clients, and a network of $n$ relays (nodes) that serve as intermediaries to unlink sender-receiver pairs. While each client has a direct connection to every relay, we model the network of relays as an $r$-regular, $\beta$-expander graph, where all relays are assumed to be synchronized. In our definitions, $N$ is polynomially bounded by the security parameter $\lambda$. Additionally, the size of the network, $n$, is bounded by: $O\left(\frac{N}{log^2 \lambda}\right)$. 

% Each input to a protocol $\Pi$ is represented as an $N$-dimensional vector. When the protocol is executed on an input $\sigma = (\sigma_1, \dots, \sigma_N)$, each $\sigma_i$ consists of a collection of properly formed message pairs. A message pair $(m,j) \in \sigma_i$ represents user $i$'s intention to send message $m$ to user $j$. Let $\mathcal{M}$ denote the (bounded) message space. Then $(m,j)$ is considered properly formed if $m \in \mathcal{M}$ and $j \in [N]$. 

% Let $\mathcal{M}(\sigma)$ represent the set of all messages in $\sigma$. It is defined as the multiset of all message pairs across all users' inputs, given by
% $$
% \mathcal{M}(\sigma_1, \dots, \sigma_N) = \bigcup_{i=1}^{N} \{ (m,j) \in \sigma_i \}.
% $$

% \subsection{Threat Model}

% Let $\Pi$ be a protocol, and let $\sigma$ be a vector of inputs to $\Pi$. Given an adversary $\mathcal{A}$, the view $\mathsf{V}^{\Pi,\mathcal{A}}(\sigma)$ of $\mathcal{A}$ consists of all the information observable by the adversary while participating in $\Pi$ on input $\sigma$, possibly with additional randomness used by the adversary to make its decisions. Given an adversary $\mathcal{A}$, the output $\mathsf{O}^{\Pi,\mathcal{A}}(\sigma) = (\mathsf{O}^{\Pi,\mathcal{A}}_1(\sigma), \dots, \mathsf{O}^{\Pi,\mathcal{A}}_N(\sigma))$ of $\Pi$ on input $\sigma$ is a vector of outputs for the $N$ parties.

% We consider the following standard adversary models, listed in increasing order of capability:\\
% \begin{itemize}
%     \item \textbf{Network adversary.} A network adversary can observe the bits flowing over every link in the network. If data is encrypted in an idealized scenario, the only information that a network adversary has is the \textit{volume} of traffic flowing over edges in the network.)\\
    
%     \item \textbf{Passive adversary.} In addition to the capabilities of a network adversary, a passive adversary can monitor the internal states and operations of a constant fraction of the relays. The adversary selects which relays to monitor non-adaptively, meaning the choice is made before the protocol begins and remains fixed throughout execution. Importantly, we assume the passive adversary has full knowledge of the network topology to inform their selection of whom to monitor.\\
    
%     \item \textbf{Active adversary.} In addition to the capabilities of a passive adversary, an active adversary can corrupt the behavior of a constant fraction of the relays. While the selection of whom to corrupt remains non-adaptive (and may be informed by the network topology), the behavior of corrupted parties can be altered on the fly. This includes deviating arbitrarily from the protocol (e.g., dropping, delaying, or repeating onions). \\
% \end{itemize}

% \subsection{Privacy Definitions}

% We define the security of a communications protocol as the difficulty for the adversary to infer who is communicating with whom (beyond what is leaked from captured messages). 

% \begin{definition} [Statistical privacy] \label{def:statistical-privacy}
%     Let $\Sigma^*$ be the input set consisting of every input of the form
%     $$
%     \sigma = \{(m_1, \pi(1)), \dots, (m_N, \pi(N))\},
%     $$
%     where $m_1, \dots, m_N \in \mathcal{M}$ and $\pi : [N] \rightarrow [N]$ is any permutation function over the set $[N]$. A communications protocol $\Pi$ is statistically private from the adversaries in the class $\mathcal{A}$ if, for all $\mathcal{A} \in \mathcal{A}$ and for all $\sigma_0, \sigma_1 \in \Sigma^*$ that differ only on the honest parties' inputs and outputs, the adversary's views $\mathsf{V}^{\Pi,\mathcal{A}}(\sigma_0)$ and $\mathsf{V}^{\Pi,\mathcal{A}}(\sigma_1)$ are statistically indistinguishable, i.e.,
%     $$
%     \Delta(\mathsf{V}^{\Pi,\mathcal{A}}(\sigma_0), \mathsf{V}^{\Pi,\mathcal{A}}(\sigma_1)) = \mathsf{negl}(\lambda),
%     $$
%     where $\lambda \in \mathbb{N}$ denotes the security parameter, and $\Delta(\cdot, \cdot)$ denotes the statistical distance (i.e., the total variation distance). Protocol $\Pi$ is perfectly secure if the statistical distance is 0.
% \end{definition}

% \begin{definition} [Distance between inputs] \label{def:distance-between-inputs} 
%     The distance between two inputs $\sigma_0 = (\sigma_1, \dots, \sigma_N)$ and $\sigma_1 = (\sigma_1', \dots, \sigma_N')$, denoted $d(\sigma_0, \sigma_1)$, is given by
%     $$
%     d(\sigma_0, \sigma_1) \overset{\Delta}{=} \sum_{i=1}^{N} |\sigma_{i} \vartriangle \sigma_{i}'|,
%     $$
%     where $\vartriangle$ represents the symmetric difference between $\sigma_{i}$ and $\sigma_{i}'$.
% \end{definition}

% \begin{definition} [Neighboring inputs] \label{def:neighboring-inputs} 
%     Two inputs $\sigma_0$ and $\sigma_1$ are neighboring if 
%     $$
%     d(\sigma_0, \sigma_1) \leq 1.
%     $$
% \end{definition}

% \subsection{Performance Metrics}

% \begin{definition} [Server load] \label{def:serverload}
%     The server load of an OR protocol $\Pi$ is defined with respect to an input vector $\sigma$ and an adversary $\mathcal{A}$. It is the expected number of onions processed by a single party in a round.\\
% \end{definition}

% \begin{definition} [Latency] \label{def:latency}
%     The latency of an OR protocol $\Pi$ is defined with respect to an input vector $\sigma$ and an adversary $\mathcal{A}$. It is the expected number of rounds in a protocol execution.
% \end{definition}

% \subsection{Properties of Expander Graphs}

% Expander graphs are a class of sparse graphs where any subset of its vertices remains well-connected to the rest of the graph, making them particularly useful in the design of network protocols.

% \begin{definition} [$\beta$-expander] \label{def:beta-expander} ~\cite[Def. 1]{10.1007/3-540-49543-6_1}
%     Formally, a graph $G = (V, E)$ is called a $\beta$-expander if, for every subset of vertices $S \subset V$ where $|S| \leq |V|/2$, the number of edges leaving $S$ (denoted by $\texttt{out}(S)$) satisfies:
    
%     $$
%     |\texttt{out}(S)| \geq \beta \cdot |S|.
%     $$
    
%     In other words, the number of edges connecting $S$ to its complement $V \setminus S$ is proportional to the size of $S$, so that that even small subsets of the graph are well-connected.\\
% \end{definition}

% % \begin{definition} [$\lambda$-absolute eiganvalue expander] \label{def:eiganvalue-expander}
% %      An $r$-regular graph $G = (V, E)$ is called a $\lambda$-absolute eiganvalue expander if, $|\lambda_2|,|\lambda_3|,...,|\lambda_n| \leq \lambda$. Here, $|\lambda_1| \geq |\lambda_2| \geq ... \geq |\lambda_n|$ are the eiganvalues of the adjacency matrix $A$ of $G$.\\

% %      In fact, iff $G$ is an $r$-regular $\beta$-expander, for some constant $\beta$, then it is also a $\lambda$-absolute eiganvalue expander where $\lambda = r - \frac{\beta^2}{2r}$ (from \textit{Cheeger's inequality}).
% % \end{definition}

% \subsubsection{Random Walks} 
    
% A \textit{random walk} on a graph is a process where, starting from an initial node, a ``walker'' moves to a neighboring node at each step according to some probability distribution over the neighbors of the current node. 

% \begin{definition} [Simple random walk] \label{def:simp-random-walk}
%     In a \textit{simple random walk} on a graph $G = (V, E)$, the walker starts at some vertex $v_0 \in V$. Let $\Gamma(v)$ denote the set of neighbors for each vertex $v \in V$. At each time step, the walker moves to a randomly chosen neighbor of the current vertex, where each neighbor is selected with equal probability. More formally, let $X_t$ denote the position of the walker at time $t$. The transition probability from vertex $v$ to vertex $u$ is given by:
    
%     $$
%     P(X_{t+1} = u \mid X_t = v) =
%     \begin{cases}
%     \frac{1}{|\Gamma(v)|} & \text{if } (v, u) \in E, \\
%     0 & \text{otherwise}.
%     \end{cases}
%     $$
    
%     Thus, at each step, the walker moves to one of the neighbors of the current vertex with equal probability.
% \end{definition}

% \begin{definition} [Stationary distribution] \label{stationary-distribution}
%     The \textit{stationary distribution} of a random walk is a probability distribution over the vertices such that if the walker is distributed according to this distribution at some time $t$, it will have the same distribution at time $t+1$. For a connected, non-bipartite graph, the random walk converges to a unique stationary distribution, and for regular graphs, this stationary distribution is uniform over all vertices.
% \end{definition}

% % \begin{definition} [Second-largest eigenvalue] \label{def:second-largest-eig}
% %     Let $\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n$ be the eigenvalues of the adjacency matrix of a $r$-regular graph. The largest eigenvalue $\lambda_1$ is equal to $r$, and the second-largest eigenvalue $\lambda_2$ (or the spectral gap $\lambda_1 - \lambda_2$) determines how quickly the random walk on the graph mixes. Specifically, the mixing time is inversely proportional to the spectral gap. A smaller value of $\lambda_2$ indicates faster mixing.

% % For a strong expander, the second-largest eigenvalue $\lambda_2$ satisfies:

% % $$
% % \lambda_2 \leq 1 - \epsilon
% % $$

% % for some constant $\epsilon > 0$. This indicates that the graph has good expansion properties, meaning that the random walk mixes rapidly. In our protocol $\Pi_x$, we take advantage of these properties of $\beta$-expanders, which allows us to prove that the routing paths chosen for onions will sufficiently mix after a polylogarithmic number of steps.
% % \end{definition}

% % \begin{definition}[Probability Distribution of a Simple Random Walk] \label{def:bound-prob-distribution}
% % Let $G = (V, E)$ be an $r$-regular $\beta$-expander graph with $|V| = n$. Let $P$ denote the transition matrix of a simple random walk on $G$, and let $h^{(t)} \in \mathbb{R}^n$ be the probability distribution vector after $t$ steps of the walk, starting from an initial distribution $h^{(0)}$. Then for any $t \geq 1$, the probability distribution vector satisfies:
% % $$
% % \| h^{(t)} - u \|_1 \leq \left(1 - \frac{\beta}{r}\right)^t,
% % $$
% % where $u = \frac{1}{n}(1, 1, \dots, 1)^T$ is the uniform distribution over $V$ and $\|\cdot\|_1$ denotes the total variation distance.
% % \end{definition}



\section{Our Protocol}

As previously established, our problem setting consists of $N$ clients (users) and $n$ relays (nodes). Assume $N = O(\mathsf{poly}(\lambda))$, and $n = O(\frac{N}{log^2 \lambda})$. While each client has a direct connection to every relay, we model the network of relays as an $r$-regular, $\beta$-expander graph, where $r > 1$. We can write $\beta = \Theta(1/r)$ \cite[Section 2.3]{Hoory2006}.\\

Each client begins with a single message they wish to send privately to another client. We assume that the set of intended recipients is a permutation of the senders, meaning that each client sends and receives exactly one message-bearing onion. At the start of the protocol, clients must download the current state of the network, which they use to construct a valid routing path of a fixed length $L + 1$ through the network. To bootstrap a path, a client selects the first hop uniformly at random from the set of all relays. The remaining path is then constructed by performing a random walk of length $L$. Finally, the recipient is appended, resulting in a path of length $L + 1$ that traverses randomly through the network and terminates at the intended recipient.\\

The client can then use this path to encrypt their message in successive layers. The innermost layer contains the message encrypted under the recipient’s public key, and each additional layer (added in reverse order of the path) specifies the next hop. Consequently, the innermost layer can only be received and decrypted by the recipient if all preceding layers are properly ``peeled'' by the nodes in the selected path.\\

At the beginning of the first round, each client sends their formed onion to the first hop. In subsequent rounds, each relay decrypts the outermost layer and then forwards the peeled onion accordingly.\\

In the final round, the recipient can then reveal the original message.

\subsection{Network Adversary}

\begin{theorem} \label{clm:network}
    $\Pi_x$ is statistically private from the network adversary when $\frac{N}{n} = \Omega(\log^2 \lambda)$ and $L = \Omega(\log^2 \lambda)$, where $\lambda \in \mathbb{N}$ denotes the security parameter.\\
\end{theorem}


\begin{proof} [Proof of Theorem~\ref{clm:network}]

For every round $t \in [L]$ and onion $o \in [N]$, we represent the adversary’s belief about the location of $o$ as a probability distribution vector:
$$
\mathbf{h}^t(o) = \Big(\mathbf{h}^t_1(o), \; \mathbf{h}^t_2(o), \; \dots, \; \mathbf{h}^t_n(o)\Big),
$$
defined over the set of nodes $[n]$. Specifically, each $\mathbf{h}^t_i(o) \in \mathbf{h}^t(o)$ denotes the adversary's best guess of the probability that $o$ is at node $i$ by the end of round $t$. Initially, the adversary observes every sender routing their onion to the first relay, so define:
$$
\mathbf{h}^1_i(o) =
\begin{cases}
    1, & \text{if $i$ is the first relay in $o$'s routing path},\\
    0, & \text{otherwise.}
\end{cases}
$$
After every round, the adversary updates their belief vector based on the observed flow of onions between nodes. Let $e^t_{i,j}$ be the number of onions that node $i$ sends to $j$ at the end of round $t$. Denote $H^t_{i,j}$ as the probability (from the adversary's point of view) that an onion at node $i$ is routed to $j$ at round $t$. Then $H^t_{i,j}$ can be expressed as the ratio:
$$
H^t_{i,j} = \frac{e^t_{i,j}}{\sum_{k \in [n]} e^t_{i,k}} = \frac{\text{number of onions routed from $i$ to $j$ at round $t$}}{\text{total number of onions routed out of $i$ at round $t$}}
$$
Thus the update rule follows a standard Markov process:
$$
\mathbf{h}^{t+1}(o) = H^t \mathbf{h}^{t}(o).
$$


\begin{lemma} \label{lem:serverload-bound}
    For every edge $(i,j)$, it follows that with overwhelming probability in $\lambda$, 
    $$
    H^t_{i,j} = \frac{1}{r} \Big(1 \pm O(\frac{1}{\log^{b} \lambda})\Big) \quad \forall \; b < \frac{1}{2}.
    $$
\end{lemma}
\begin{proof}[Proof of Lemma \ref{lem:serverload-bound}]
In an $r$–regular graph the number of paths of length $t$ starting at any given node is exactly $r^t$, and this implies that the number of such paths that end at any fixed node $i$ is the same regardless of $i$. 
Now suppose we model the first $t$ nodes of an onion's routing path as choosing uniformly at random one of the $n \cdot r^t$ paths of length $t$. Because the number of paths landing at any given node is the same, it follows that the probability that an onion's path ends by traversing edge $(i,j)$ after $t$ rounds is exactly $\frac{1}{nr}$.

Let $e^t_{i,j}$ denote the number of onions routed form $i$ to $j$ at the end of round $t$. By applying lemma \ref{lem:chernoff-deviation}, with overwhelming probability in $\lambda$, for any edge $(i,j)$ in any given round:
$$
\Big|e^t_{i,j} - \frac{N}{nr}\Big| \leq \sqrt{\frac{3N}{nr}(\log^c \lambda + \log n + \log r)}, \quad \forall c > 1.
$$
Now let $S^t_i = \sum_{j \in \mathcal{N}(i)} e^t_{i,j} = \sum_{j \in \mathcal{N}(i)} e^{t - 1}_{j,i}$ denote the server load at node $i$ in round $t$. Also applying lemma \ref{lem:chernoff-deviation}, with overwhelming probability in $\lambda$, for any $i \in [n]$ in any given round:
$$
\Big|S^t_i - \frac{N}{n}\Big| \leq \sqrt{\frac{3N}{n}(\log^d \lambda + \log n)}, \quad \forall d > 1.
$$
Therefore with overwhelming probability:
\begin{align*}
\frac{e^t_{i,j}}{S^t_i} &= \frac{1}{r}\Big(\frac{1 \pm O(\sqrt{\frac{3nr}{N}(\log^c \lambda + \log n + \log r)})}{1 \mp O(\sqrt{\frac{3n}{N}(\log^d \lambda + \log n)})}\Big)\\
                        &= \frac{1}{r} \cdot \Big(\frac{1 \pm O(\sqrt{\log^{c - 2} \lambda})}{1 \mp O(\sqrt{\log^{d - 2} \lambda})}\Big)\\
                        &= \frac{1}{r} \cdot \Big(1 \pm O(\log^{\frac{c - 2}{2}} \lambda)\Big) &\forall \; 1 < d < 2\\
                        &= \frac{1}{r} \cdot \Big(1 \pm O(\frac{1}{\log^{b} \lambda}\Big)) &\forall b < \frac{1}{2}\\
\end{align*}

\end{proof}
Let $A$ be the normalized adjacency matrix of $G$ (i.e., the transition matrix of a random walk), where:
$$
A_{i,j} = \begin{cases}
\frac{1}{r}, & j \in \mathcal{N}(i),\\[1mm]
0, & \text{otherwise.}
\end{cases}
$$
Then for every round $t$ we can write:
$$
H^t = A \cdot \Bigl(1 \pm \delta\Bigr), \quad \text{ with $\delta = O\Bigl(\frac{1}{\log^{b} \lambda}\Bigr)$, and $b < \frac{1}{2}$}
$$
Define
$$
E_t = H^t H^{t-1} \cdots H^1.
$$
Then we have:
$$
A^t \Bigl(1 - \delta\Bigr)^t \le E_t \le A^t \Bigl(1 + \delta\Bigr)^t.
$$
Now suppose that onion $O_1$ is generated at node $k$ and $O_2$ at node $k'$. Define the initial probability vectors, $\mathbf{u}_k$ and $\mathbf{u}_k'$ respectively:
$$
\mathbf{u}_k =
\begin{cases}
    1, & \text{if $i = k$},\\
    0, & \text{otherwise.}
\end{cases}
$$
$$
\mathbf{u}_k' =
\begin{cases}
    1, & \text{if $i = k'$},\\
    0, & \text{otherwise.}
\end{cases}
$$
Then
$$
\mathbf{h}^t(O_1) = E_t \mathbf{u}_k,\quad \mathbf{h}^t(O_2) = E_t \mathbf{u}_{k'}.
$$
$$
\Rightarrow \mathbf{h}^t(O_1) - \mathbf{h}^t(O_2) = E_t \bigl( \mathbf{u}_k - \mathbf{u}_{k'} \bigr) \leq A^t \bigl( \mathbf{u}_k - \mathbf{u}_{k'} \bigr) (1 + \delta)^t
$$
Because the ideal random walk on an $r$-regular graph is symmetric, its stationary distribution is uniform. Moreover, writing $\mathbf{u}_k - \mathbf{u}_{k'}$ in the eigenbasis of $A$, the projection onto the first eigenvector (which corresponds to the stationary distribution) is 0. By Lemma \ref{lem:stochastic}, there exists some constant $\gamma < 1$ such that $(1 + \delta)\gamma < 1$ and:
$$
A^t (\mathbf{u}_k - \mathbf{u}_{k'}) \le \gamma^t \bigl| \mathbf{u}_k - \mathbf{u}_{k'} \bigr|.
$$
We obtain
$$
\bigl| \mathbf{h}^t(O_1) - \mathbf{h}^t(O_2) \bigr| \le \Bigl(1 + \delta\Bigr)^t\, \gamma^t\, \bigl| \mathbf{u}_k - \mathbf{u}_{k'} \bigr|.
$$
For $t = \log^2 \lambda$, 
$$
\bigl| \mathbf{h}^t(O_1) - \mathbf{h}^t(O_2) \bigr| = \text{negl}(\lambda).
$$
Therefore the adversary’s best estimates for the locations of $O_1$ and $O_2$ are statistically indistinguishable, and the protocol $\Pi_x$ is statistically private with respect to the network adversary.

\end{proof}


\begin{lemma} \label{lem:stochastic}
Let $h = (h_1, \dots, h_n)^T$ be a stochastic vector representing a probability distribution, and let $A$ be the transition probability matrix of a random walk on an $r$-regular $\beta$-expander graph. For some constant $\gamma < 1$, and for all $t \geq 1$, the distribution after $t$ steps satisfies:
$$
A^t h \leq \frac{1}{n} \mathbf{I} + \gamma^t \mathbf{I},
$$
where $\mathbf{I}$ is the vector of all ones.
\end{lemma}
\begin{proof} [Proof for Lemma \ref{lem:stochastic}]

$A$ has an eigenvector base, $v_1,\dots,v_n$ with eogenvalues $\gamma_1,\dots, \gamma_n$.

Any vector $h$ can be written as
$h=\sum c_1 v_i$

$Ah= \sum c_i\gamma_1 v_i$

If $h$ is stochastic, then $\gamma_1=1$

So 

$A^t h= 1/nI + \sum_{i\geq 2} c_i^t v_i$

All $|\lambda_i| \leq \lambda_2< 1$


    
\end{proof}



\clearpage

\subsection{Passive Adversary}

\begin{theorem} \label{clm:passive}
    $\Pi_x$ is statistically private against a passive adversary who can monitor up to a constant fraction $\chi < \frac{1}{2}$ of the servers. Assume $\frac{N}{n} = \Omega(\log^2 \lambda)$ and $L = \Omega(\log^2 \lambda)$, where $\lambda \in \mathbb{N}$ denotes the security parameter.\\
\end{theorem}

\begin{proof} [Proof of Theorem~\ref{clm:passive}]

Let $C$ denote the set of monitored nodes, where $|C| = \chi < \frac{n}{2}$. Similar to the network adversary setup, at the start of the protocol, the adversary knows the exact location of every sender's onion. Let $O$ represent one of these onions, and let $S \in [n]$ be its initial location. We will show that with overwhelming probability in $\lambda$, $O$ has at least $\Omega(\log^2 \lambda)$ honest parties in its routing path. 


\begin{lemma}\label{lem:escape-time}
Let $C \subset [n]$ be any subset of relays with $|C| < \frac{n}{2}$. Then with overwhelming probability in $\lambda$, a random walk of length $\Omega(\log^c \lambda)$ for $c > 1$ will visit $\Omega(\log^c \lambda)$ relays $\not\in C$ with overwhelming probability in $\lambda$.
\end{lemma}
\begin{proof}[Proof for Lemma \ref{lem:escape-time}]
Let $t_n$ be the number of visits to $C$ in $t$ steps, where $\mathbb{E}[t_n] = \frac{|C|t}{n}$. Then \cite[Theorem 2.1]{gillman1998chernoff} states that for any deviation $\gamma > 0$:
$$
\Pr[t_n - \frac{t|C|}{n} \geq \gamma] \leq (1 + \gamma \epsilon / 10t) N_q e^{-\gamma^2 \epsilon / 20t}.
$$
where $N_q = 1$ accounts for the starting distribution, and $\epsilon = 1 - \lambda_2$ is the spectral gap of the expander. Then we can write:
\begin{align*}
    \Pr[t_n - \frac{t|C|}{n} \geq \gamma] \leq (1 + \frac{\gamma \cdot \Theta\left(\frac{1}{r}\right)}{10t}) \exp(-\frac{\gamma^2 \cdot \Theta\left(\frac{1}{r}\right)}{20t})
\end{align*}
Requiring $\Pr[t_n - \frac{t|C|}{n} \geq \gamma] = \mathsf{negl}(\lambda)$,
\begin{align*}
    \frac{\gamma^2 \cdot \Theta\left(\frac{1}{r}\right)}{20t} &= \log^\delta \lambda &\text{for any $\delta > 1$}\\
    \Rightarrow \gamma &= \sqrt{t \cdot \log^\delta \lambda}
\end{align*}
Thus with overwhelming probability in $\lambda$, 
\begin{align*}
    t_n &< \frac{t|C|}{n} + \sqrt{t \cdot \log^\delta \lambda}\\
    &< \frac{t}{2} + \sqrt{t \cdot \log^\delta \lambda}\\
    t - t_n &\geq \frac{t}{2} - \sqrt{t \cdot \log^\delta \lambda}\\
\end{align*}
This gives a bound for the number of visits $\not\in C$. Specifically when $t = \Omega(\log^c \lambda)$ for $c > 1$
\begin{align*}
    t - t_n &= \Omega(\log^c \lambda - \log^{\frac{c + \delta}{2}}\lambda)\\
    &= \Omega((\log^c \lambda)(1 - \log^{\frac{\delta - c}{2}}\lambda))\\
\end{align*}
Since $c > 1$, a valid choice of $\delta$ is $\delta = \frac{c + 1}{2}$, which implies:
$$
\frac{\delta - c}{2} < 0.
$$
Therefore:
\begin{align*}
    t - t_n &= \Omega(\log^c \lambda) \cdot \Theta(1 - \log^{\frac{\delta - c}{2}}\lambda)\\
    &= \Omega(\log^c \lambda) \cdot \Theta(1)\\
\end{align*}
We conclude that after $t = \Omega(\log^c \lambda)$ steps, a random walk will visit $\Omega(\log^c \lambda)$ vertices $\not\in C$ with overwhelming probability in $\lambda$.
\end{proof}

    
\end{proof}


\section{Conclusion and Open Problems}


\clearpage
% ~~~ REFERENCES ~~~ %
%\bibliographystyle{plain}
%\bibliographystyle{is-alpha}
\bibliographystyle{stylefiles/alpha-short}
\bibliography{template/bibfiles/abbrev3,template/bibfiles/crypto,template/bibfiles/anon,template/bibfiles/refs} 

\appendix 

\clearpage

% MORE PROOFS
\section{Supplementary proofs} \label{sec:proofs}

\begin{lemma} \label{lem:chernoff-deviation} Consider a random experiment with $n$ equally likely outcomes conducted over $N$ independent Poisson trials. Let $\lambda$ denote any sufficiently large natural number. Let $X_i$ denote the number of times an outcome $i$ occurred after $N$ trials. Then with overwhelming probability w.r.t. $\lambda$, every outcome $i \in [n]$ will deviate from $\mathbb{E}[X_i] = \frac{N}{n}$ no more than:
$$
\sqrt{\frac{3N}{n}(\log^c \lambda + \log n)}
$$
for all $c > 1$
\end{lemma} 

\begin{proof}[Proof of Lemma~\ref{lem:chernoff-deviation}]

Let $X_i$ denote the number of times some outcome $i \in [n]$ occurred after $N$ trials. Then $\mathbb{E}[X_i]$ = $\frac{N}{n}$, and it follows from Chernoff bounds~\cite[Cor.~4.6]{MU05} that for all $0 < \delta < 1$:
$$
\Pr\left(|X_i - \frac{N}{n}| \geq \delta \cdot \frac{N}{n} \right) \leq 2 e^{-\frac{\delta^2 \cdot N}{3n}}.
$$
Applying the union bound over all $n$ outcomes, we get:
$$
\Pr\left(\exists i \in [n]: |X_i - \frac{N}{n}| \geq \delta \cdot \frac{N}{n} \right) \leq n \cdot 2 e^{-\frac{\delta^2 \cdot N}{3n}} = 2 e^{\ln n -\frac{\delta^2 \cdot N}{3n}}.
$$
Solve for $\delta$, requiring the probability to be negligible in $\lambda$:

\begin{align*}
\frac{\delta^2 \cdot N}{3n} - \log n &\geq \log^c \lambda, &\forall c > 1\\
\delta &\geq \sqrt{\frac{3n}{N}(\log^c \lambda + \log n)} &
\end{align*}

Thus, with overwhelming probability (w.r.t. $\lambda$), we have:
\begin{align*}
    \left|X_i - \frac{N}{n}\right| < \sqrt{\frac{3N}{n}(\log^c \lambda + \log n)} \quad \forall c > 1
\end{align*}

\end{proof}


\end{document}